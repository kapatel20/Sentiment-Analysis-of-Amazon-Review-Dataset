{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97066656",
   "metadata": {},
   "source": [
    "#### Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4bb4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import gensim\n",
    "import contractions\n",
    "import torch\n",
    "import torchvision\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ade4b",
   "metadata": {},
   "source": [
    "# Q - 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e95730",
   "metadata": {},
   "source": [
    "#### Generating the same 60k data used in HW1 using the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8427ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.tsv -> Entire Amazon Review Dataset. Should be in the current directory while running\n",
    "df = pd.read_table(\"data.tsv\",on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e83ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataF = df[[\"product_title\",\"review_body\",\"star_rating\"]] \n",
    "dataF.dropna(inplace=True)\n",
    "dataF[\"star_rating\"] = dataF[\"star_rating\"].apply(lambda x:int(x))\n",
    "\n",
    "dataF[\"rating\"] = dataF[\"star_rating\"]\n",
    "dataF[\"rating\"].mask(dataF[\"rating\"]<=2,1,inplace=True)\n",
    "dataF[\"rating\"].mask(dataF[\"rating\"]==3,2,inplace=True)\n",
    "dataF[\"rating\"].mask(dataF[\"rating\"]>=4,3,inplace=True)\n",
    "\n",
    "df_1 = dataF[dataF[\"rating\"]==1]\n",
    "df_2 = dataF[dataF[\"rating\"]==2]\n",
    "df_3 = dataF[dataF[\"rating\"]==3]\n",
    "class_1 = df_1.sample(n=20000, random_state=0)\n",
    "class_2 = df_2.sample(n=20000, random_state=0)\n",
    "class_3 = df_3.sample(n=20000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feeecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the subset of 60k data for future use\n",
    "final_df = pd.concat([class_1,class_2,class_3],ignore_index=True)\n",
    "final_df.to_csv(\"final_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8fc4c0",
   "metadata": {},
   "source": [
    "#### Generating Validation dataset for Model evaluation (Validation set does not contain any data from training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc5b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_val = dataF.drop(index=list(class_1.index))[dataF[\"rating\"]==1]\n",
    "class_1_val = df_1_val.sample(n=4000, random_state=0)\n",
    "df_2_val = dataF.drop(index=list(class_2.index))[dataF[\"rating\"]==2]\n",
    "class_2_val = df_2_val.sample(n=4000, random_state=0)\n",
    "df_3_val = dataF.drop(index=list(class_3.index))[dataF[\"rating\"]==3]\n",
    "class_3_val = df_3_val.sample(n=4000, random_state=0)\n",
    "final_df_val = pd.concat([class_1_val,class_2_val,class_3_val],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2105d8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bare Escentuals Sugared Strawberry 100% Natura...</td>\n",
       "      <td>This smelled awful. When I brought it into a S...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dabur Vatika Sweet Almond Shampoo 400mL</td>\n",
       "      <td>Made my hair fall out</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Philips Norelco Rq12+ Replacement Head For Ser...</td>\n",
       "      <td>RQ12/52 head is much better than this thing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Philips Norelco Electric Razor Shaver, Model 6...</td>\n",
       "      <td>I find it interesting that all the reviews up ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bain-De-Terre Recovery Complex Anti-Frizz Seru...</td>\n",
       "      <td>Not the product I remembered. Sad but true. Re...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>China Glaze Nail Lacquer, Escaping Reality, 0....</td>\n",
       "      <td>I'm a huge CG fan and when I saw this color on...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>Jolie Mineral Sheer Tint SPF 20 Tinted Moistur...</td>\n",
       "      <td>This is one of the best products to use as a t...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>Flat Top Kabuki Foundation Brush By Keshima - ...</td>\n",
       "      <td>Best Kabuki brush I have ever owned! Easily wo...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>Home Health Hyaluronic Acid Cream</td>\n",
       "      <td>a good cream.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>Cirepil Pre-Depilatory Oil</td>\n",
       "      <td>Best line for waxing!</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           product_title  \\\n",
       "0      Bare Escentuals Sugared Strawberry 100% Natura...   \n",
       "1                Dabur Vatika Sweet Almond Shampoo 400mL   \n",
       "2      Philips Norelco Rq12+ Replacement Head For Ser...   \n",
       "3      Philips Norelco Electric Razor Shaver, Model 6...   \n",
       "4      Bain-De-Terre Recovery Complex Anti-Frizz Seru...   \n",
       "...                                                  ...   \n",
       "11995  China Glaze Nail Lacquer, Escaping Reality, 0....   \n",
       "11996  Jolie Mineral Sheer Tint SPF 20 Tinted Moistur...   \n",
       "11997  Flat Top Kabuki Foundation Brush By Keshima - ...   \n",
       "11998                  Home Health Hyaluronic Acid Cream   \n",
       "11999                         Cirepil Pre-Depilatory Oil   \n",
       "\n",
       "                                             review_body  star_rating  rating  \n",
       "0      This smelled awful. When I brought it into a S...            1       1  \n",
       "1                                  Made my hair fall out            1       1  \n",
       "2            RQ12/52 head is much better than this thing            1       1  \n",
       "3      I find it interesting that all the reviews up ...            2       1  \n",
       "4      Not the product I remembered. Sad but true. Re...            1       1  \n",
       "...                                                  ...          ...     ...  \n",
       "11995  I'm a huge CG fan and when I saw this color on...            4       3  \n",
       "11996  This is one of the best products to use as a t...            5       3  \n",
       "11997  Best Kabuki brush I have ever owned! Easily wo...            5       3  \n",
       "11998                                      a good cream.            5       3  \n",
       "11999                              Best line for waxing!            5       3  \n",
       "\n",
       "[12000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92f510d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Hansen Lavender Spa Wax Remover Kit For ...</td>\n",
       "      <td>For the love of all that is holy, DON'T buy th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BDS - Lovely Fresh Big Bow Headband Makeup Fac...</td>\n",
       "      <td>it looks cute and soft but way too tight which...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Professional Studio Quality 12 Piece Natural C...</td>\n",
       "      <td>They don't pick up powders very well especiall...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dolce &amp; Gabbana Light Blue 3.4 Oz For Women, B...</td>\n",
       "      <td>Is not original!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conair Curl Innovation Jumbo Hot Rollers with ...</td>\n",
       "      <td>Did not like these at all. They became so hot ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>3 Piece, Genuine Czech, Etched, Crystal Glass ...</td>\n",
       "      <td>Will never use a cardboard or metal nail file ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>Cosrx Bha Blackhead Power Liquid</td>\n",
       "      <td>This is hands down the best BHA product I've e...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>Coppertone ultraGUARD Sunscreen</td>\n",
       "      <td>I love this sunscreen because of the smell, re...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>Palmers Cocoa Butter With E &amp; Alpha Beta Smoot...</td>\n",
       "      <td>I used this during warmer parts of the year an...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>Groom Mate Platinum XL Nose &amp; Ear Hair Trimmer...</td>\n",
       "      <td>I bought this as a gift for my husband when he...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           product_title  \\\n",
       "0      Sally Hansen Lavender Spa Wax Remover Kit For ...   \n",
       "1      BDS - Lovely Fresh Big Bow Headband Makeup Fac...   \n",
       "2      Professional Studio Quality 12 Piece Natural C...   \n",
       "3      Dolce & Gabbana Light Blue 3.4 Oz For Women, B...   \n",
       "4      Conair Curl Innovation Jumbo Hot Rollers with ...   \n",
       "...                                                  ...   \n",
       "59995  3 Piece, Genuine Czech, Etched, Crystal Glass ...   \n",
       "59996                   Cosrx Bha Blackhead Power Liquid   \n",
       "59997                    Coppertone ultraGUARD Sunscreen   \n",
       "59998  Palmers Cocoa Butter With E & Alpha Beta Smoot...   \n",
       "59999  Groom Mate Platinum XL Nose & Ear Hair Trimmer...   \n",
       "\n",
       "                                             review_body  star_rating  rating  \n",
       "0      For the love of all that is holy, DON'T buy th...            1       1  \n",
       "1      it looks cute and soft but way too tight which...            2       1  \n",
       "2      They don't pick up powders very well especiall...            2       1  \n",
       "3                                       Is not original!            1       1  \n",
       "4      Did not like these at all. They became so hot ...            1       1  \n",
       "...                                                  ...          ...     ...  \n",
       "59995  Will never use a cardboard or metal nail file ...            5       3  \n",
       "59996  This is hands down the best BHA product I've e...            5       3  \n",
       "59997  I love this sunscreen because of the smell, re...            5       3  \n",
       "59998  I used this during warmer parts of the year an...            5       3  \n",
       "59999  I bought this as a gift for my husband when he...            5       3  \n",
       "\n",
       "[60000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the csv file created in the above section\n",
    "data = pd.read_csv(\"final_data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748d874",
   "metadata": {},
   "source": [
    "#### Using the same steps for pre-processing used in HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "213fc007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Contraction which will use fix function of contraction module to perform the task.\n",
    "def expandContractions(text):\n",
    "    exp_words = []\n",
    "    for w in text.split():\n",
    "        exp_words.append(contractions.fix(w))\n",
    "    return ' '.join(exp_words)\n",
    "\n",
    "# Using re to get rid of html tags and hyperlinks\n",
    "def preprocessText(x):\n",
    "    x = re.sub(r'<.*>','',x)\n",
    "    x = re.sub(r'http[s]?://\\S+', '', x)\n",
    "    return x\n",
    "\n",
    "# Implementing tokenization of review data using TweetTokenizer which is very similar to nltk.word_tokenize() but gives better \n",
    "# results here.\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "def tokenize(x):\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "# Function that used WordNetLemmatizer() from nltk to lemmatize data.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lem(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i] = lemmatizer.lemmatize(y[i])\n",
    "    return y\n",
    "\n",
    "data[\"preprocessed_reviews\"] = data[\"review_body\"].apply(lambda x:x.lower())\n",
    "data[\"preprocessed_reviews\"] = data[\"preprocessed_reviews\"].apply(expandContractions)\n",
    "data[\"preprocessed_reviews\"] = data[\"preprocessed_reviews\"].apply(preprocessText)\n",
    "data[\"preprocessed_reviews\"] = data[\"preprocessed_reviews\"].apply(tokenize)\n",
    "data[\"preprocessed_reviews\"] = data[\"preprocessed_reviews\"].apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78612899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>rating</th>\n",
       "      <th>preprocessed_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Hansen Lavender Spa Wax Remover Kit For ...</td>\n",
       "      <td>For the love of all that is holy, DON'T buy th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[for, the, love, of, all, that, is, holy, ,, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BDS - Lovely Fresh Big Bow Headband Makeup Fac...</td>\n",
       "      <td>it looks cute and soft but way too tight which...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[it, look, cute, and, soft, but, way, too, tig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Professional Studio Quality 12 Piece Natural C...</td>\n",
       "      <td>They don't pick up powders very well especiall...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[they, do, not, pick, up, powder, very, well, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dolce &amp; Gabbana Light Blue 3.4 Oz For Women, B...</td>\n",
       "      <td>Is not original!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[is, not, original, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conair Curl Innovation Jumbo Hot Rollers with ...</td>\n",
       "      <td>Did not like these at all. They became so hot ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[did, not, like, these, at, all, ., they, beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>3 Piece, Genuine Czech, Etched, Crystal Glass ...</td>\n",
       "      <td>Will never use a cardboard or metal nail file ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[will, never, use, a, cardboard, or, metal, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>Cosrx Bha Blackhead Power Liquid</td>\n",
       "      <td>This is hands down the best BHA product I've e...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[this, is, hand, down, the, best, bha, product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>Coppertone ultraGUARD Sunscreen</td>\n",
       "      <td>I love this sunscreen because of the smell, re...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[i, love, this, sunscreen, because, of, the, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>Palmers Cocoa Butter With E &amp; Alpha Beta Smoot...</td>\n",
       "      <td>I used this during warmer parts of the year an...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[i, used, this, during, warmer, part, of, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>Groom Mate Platinum XL Nose &amp; Ear Hair Trimmer...</td>\n",
       "      <td>I bought this as a gift for my husband when he...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>[i, bought, this, a, a, gift, for, my, husband...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           product_title  \\\n",
       "0      Sally Hansen Lavender Spa Wax Remover Kit For ...   \n",
       "1      BDS - Lovely Fresh Big Bow Headband Makeup Fac...   \n",
       "2      Professional Studio Quality 12 Piece Natural C...   \n",
       "3      Dolce & Gabbana Light Blue 3.4 Oz For Women, B...   \n",
       "4      Conair Curl Innovation Jumbo Hot Rollers with ...   \n",
       "...                                                  ...   \n",
       "59995  3 Piece, Genuine Czech, Etched, Crystal Glass ...   \n",
       "59996                   Cosrx Bha Blackhead Power Liquid   \n",
       "59997                    Coppertone ultraGUARD Sunscreen   \n",
       "59998  Palmers Cocoa Butter With E & Alpha Beta Smoot...   \n",
       "59999  Groom Mate Platinum XL Nose & Ear Hair Trimmer...   \n",
       "\n",
       "                                             review_body  star_rating  rating  \\\n",
       "0      For the love of all that is holy, DON'T buy th...            1       1   \n",
       "1      it looks cute and soft but way too tight which...            2       1   \n",
       "2      They don't pick up powders very well especiall...            2       1   \n",
       "3                                       Is not original!            1       1   \n",
       "4      Did not like these at all. They became so hot ...            1       1   \n",
       "...                                                  ...          ...     ...   \n",
       "59995  Will never use a cardboard or metal nail file ...            5       3   \n",
       "59996  This is hands down the best BHA product I've e...            5       3   \n",
       "59997  I love this sunscreen because of the smell, re...            5       3   \n",
       "59998  I used this during warmer parts of the year an...            5       3   \n",
       "59999  I bought this as a gift for my husband when he...            5       3   \n",
       "\n",
       "                                    preprocessed_reviews  \n",
       "0      [for, the, love, of, all, that, is, holy, ,, d...  \n",
       "1      [it, look, cute, and, soft, but, way, too, tig...  \n",
       "2      [they, do, not, pick, up, powder, very, well, ...  \n",
       "3                                 [is, not, original, !]  \n",
       "4      [did, not, like, these, at, all, ., they, beca...  \n",
       "...                                                  ...  \n",
       "59995  [will, never, use, a, cardboard, or, metal, na...  \n",
       "59996  [this, is, hand, down, the, best, bha, product...  \n",
       "59997  [i, love, this, sunscreen, because, of, the, s...  \n",
       "59998  [i, used, this, during, warmer, part, of, the,...  \n",
       "59999  [i, bought, this, a, a, gift, for, my, husband...  \n",
       "\n",
       "[60000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88405511",
   "metadata": {},
   "source": [
    "#### Applying same set of preprocessing to validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4bc0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_val[\"preprocessed_reviews\"] = final_df_val[\"review_body\"].apply(lambda x:x.lower())\n",
    "final_df_val[\"preprocessed_reviews\"] = final_df_val[\"preprocessed_reviews\"].apply(expandContractions)\n",
    "final_df_val[\"preprocessed_reviews\"] = final_df_val[\"preprocessed_reviews\"].apply(preprocessText)\n",
    "final_df_val[\"preprocessed_reviews\"] = final_df_val[\"preprocessed_reviews\"].apply(tokenize)\n",
    "final_df_val[\"preprocessed_reviews\"] = final_df_val[\"preprocessed_reviews\"].apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750f18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2305c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a15a62",
   "metadata": {},
   "source": [
    "# Q - 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc388024",
   "metadata": {},
   "source": [
    "#### a) Loading the pretrained “word2vec-google-news-300” Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ae38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - 1\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d88007d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "result1 = wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28f3c07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shampoo', 0.6024006009101868)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['hair', 'moisturizer'], negative=['skin'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72c2aa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hair', 0.4457809627056122)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['scalp', 'shampoo'], negative=['conditioner'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf9994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3085579",
   "metadata": {},
   "source": [
    "#### b) Training word2vec model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9690e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=data[\"preprocessed_reviews\"],min_count=9,vector_size=300,window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f458c9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conditioner', 0.7012938261032104)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['hair', 'moisturizer'], negative=['skin'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "616b60e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dandruff', 0.618808388710022)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['scalp', 'shampoo'], negative=['conditioner'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b1800",
   "metadata": {},
   "source": [
    "<b><i>What do you conclude from comparing vectors generated by yourself and the pretrained model?</i></b>\n",
    "\n",
    "<i>Ans. </i> The output generated by myself and pretrained model are quite similar for the above 2 examples. As it can be seen, \"shampoo\" and \"conditioner\" quite similar for hair. Also \"hair\" and \"dandruff\" both are associated with the scalp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21f48a",
   "metadata": {},
   "source": [
    "<b><i>Which of the Word2Vec models seems to encode semantic similarities between words better?</i></b>\n",
    "\n",
    "<i>Ans. </i>Both the models are working well on the above examples but in general pretrained word2vec model will be better because it is trained on more data. If we try to use the example of king, man and woman to get answer as queen, the model trained on the particular data gives \"Word not found error\". So Google-News-300 word2vec is better as it will handle new words in Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2fcd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae631ac6",
   "metadata": {},
   "source": [
    "#### Train Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00aac131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the tokenized reviews of training data into its vector using word2vec-google-news-300\n",
    "remove_list = []\n",
    "final_embeddings = []\n",
    "mean_embeddings = []\n",
    "for i in range(len(data)):\n",
    "    word_embeddings = []\n",
    "        \n",
    "    for word in data[\"preprocessed_reviews\"][i]:\n",
    "        try:\n",
    "            word_embeddings.append(wv[word])\n",
    "        except KeyError:\n",
    "            word_embeddings.append(np.zeros(300))\n",
    "    final_embeddings.append(word_embeddings)\n",
    "    mean_embeddings.append(np.mean(word_embeddings,axis=0))\n",
    "\n",
    "# Pretrained_Embeddings_Reviews -> Contains vectors for all the words in the review\n",
    "data[\"pretrained_embeddings_reviews\"] = final_embeddings\n",
    "\n",
    "# mean_embeddings -> Contains the mean of all the words for a review\n",
    "data[\"mean_embeddings\"]  = mean_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9676b24",
   "metadata": {},
   "source": [
    "#### Validation Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dfab480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the tokenized reviews of validation data into its vector using word2vec-google-news-300\n",
    "remove_list_val = []\n",
    "final_embeddings_val = []\n",
    "mean_embeddings_val = []\n",
    "\n",
    "for i in range(len(final_df_val)):\n",
    "    word_embeddings_val = []\n",
    "        \n",
    "    for word in final_df_val[\"preprocessed_reviews\"][i]:\n",
    "        try:\n",
    "            word_embeddings_val.append(wv[word])\n",
    "        except KeyError:\n",
    "            word_embeddings_val.append(np.zeros(300))\n",
    "\n",
    "    final_embeddings_val.append(word_embeddings_val)\n",
    "    mean_embeddings_val.append(np.mean(word_embeddings_val,axis=0))\n",
    "\n",
    "# Pretrained_Embeddings_Reviews -> Contains vectors for all the words in the review\n",
    "final_df_val[\"pretrained_embeddings_reviews\"] = final_embeddings_val\n",
    "\n",
    "# mean_embeddings -> Contains the mean of all the words for a review\n",
    "final_df_val[\"mean_embeddings\"]  = mean_embeddings_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56b175f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.027972835964626738, 0.03650338914659288, 0....\n",
       "1        [-0.0003587676257621951, 0.03917959259777534, ...\n",
       "2        [0.05609130859375, 0.03489990234375, 0.0506408...\n",
       "3        [0.03960418701171875, -0.04815673828125, 0.130...\n",
       "4        [0.048212076822916665, 0.030977147420247396, 0...\n",
       "                               ...                        \n",
       "59995    [0.04057728160511364, 0.044389204545454544, -0...\n",
       "59996    [0.011457722981770833, 0.03865999221801758, 0....\n",
       "59997    [0.014290771484375, 0.0109033203125, 0.0166210...\n",
       "59998    [0.017403602600097656, 0.04505568742752075, 0....\n",
       "59999    [0.057936295219089676, 0.014142741327700407, 0...\n",
       "Name: mean_embeddings, Length: 60000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"mean_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac782ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.109375, 0.140625, -0.03173828, 0.16601562,...\n",
       "1        [[-0.055908203, 0.11767578, 0.2109375, 0.00836...\n",
       "2        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "3        [[-0.22558594, -0.01953125, 0.09082031, 0.2373...\n",
       "4        [[0.08496094, -0.095214844, 0.119140625, 0.111...\n",
       "                               ...                        \n",
       "11995    [[-0.22558594, -0.01953125, 0.09082031, 0.2373...\n",
       "11996    [[0.109375, 0.140625, -0.03173828, 0.16601562,...\n",
       "11997    [[-0.12695312, 0.021972656, 0.28710938, 0.1533...\n",
       "11998    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "11999    [[-0.12695312, 0.021972656, 0.28710938, 0.1533...\n",
       "Name: pretrained_embeddings_reviews, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_val[\"pretrained_embeddings_reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a25c497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting if there is any null values in the reviews of train data after text-preprocessing \n",
    "i=0\n",
    "delete = []\n",
    "for rv in data['preprocessed_reviews']:\n",
    "    if(len(rv) == 0):\n",
    "        delete.append(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04a03039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting if there is any null values in the reviews of validation data after text-preprocessing\n",
    "i=0\n",
    "delete_val = []\n",
    "for rv in final_df_val['preprocessed_reviews']:\n",
    "    if(len(rv) == 0):\n",
    "        delete_val.append(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "386535cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null Values in Train Data :  1\n",
      "Number of Null values in validation data :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Null Values in Train Data : \", len(delete))\n",
    "print(\"Number of Null values in validation data : \",len(delete_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "feba21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(index=delete,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567d219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbe73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "078039b7",
   "metadata": {},
   "source": [
    "# 3.  Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c599fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for applying TF-IDF vectorizer\n",
    "data[\"tfidf_review\"] = data[\"review_body\"]\n",
    "data[\"tfidf_review\"] = data[\"tfidf_review\"].apply(lambda x:x.lower())\n",
    "data[\"tfidf_review\"] = data[\"tfidf_review\"].apply(expandContractions)\n",
    "data[\"tfidf_review\"] = data[\"tfidf_review\"].apply(preprocessText)\n",
    "data[\"tfidf_review\"] = data[\"tfidf_review\"].apply(tokenize)\n",
    "data[\"tfidf_review\"] = data[\"tfidf_review\"].apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e65ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinList(t):\n",
    "    return ' '.join(t)\n",
    "data[\"tfidf_review\"] = data[\"tfidf_review\"].apply(joinList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2012c561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        for the love of all that is holy , do not buy ...\n",
       "1        it look cute and soft but way too tight which ...\n",
       "2        they do not pick up powder very well especiall...\n",
       "3                                        is not original !\n",
       "4        did not like these at all . they became so hot...\n",
       "                               ...                        \n",
       "59995    will never use a cardboard or metal nail file ...\n",
       "59996    this is hand down the best bha product i have ...\n",
       "59997    i love this sunscreen because of the smell , r...\n",
       "59998    i used this during warmer part of the year and...\n",
       "59999    i bought this a a gift for my husband when he ...\n",
       "Name: tfidf_review, Length: 59999, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tfidf_review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f579b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(data[\"tfidf_review\"],data['rating'], stratify=data['rating'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75a09d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Tf-Idf vectorizer to the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "text = list(X_train_tfidf.values)+list(X_test_tfidf.values)\n",
    "vec.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12d67651",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_TFIDF = vec.transform(X_train_tfidf)\n",
    "X_TEST_TFIDF = vec.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff534e0",
   "metadata": {},
   "source": [
    "#### Perceptron Model with TF-IDF as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "123b2fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perceptron Model implemented using scikit-learn\n",
    "prc_tfidf = Perceptron(random_state=5)\n",
    "prc_tfidf.fit(X_TRAIN_TFIDF,y_train_tfidf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85a4ac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Percetron with TF-IDF as embeddings :  62.4\n"
     ]
    }
   ],
   "source": [
    "y_pred_tfidf = prc_tfidf.predict(X_TEST_TFIDF)\n",
    "print(\"Accuracy score for Percetron with TF-IDF as embeddings : \",accuracy_score(y_test_tfidf,y_pred_tfidf)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118dbd5e",
   "metadata": {},
   "source": [
    "#### SVM Model with TF-IDF as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "095bae81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC with TF-IDF Embeddings :  71.26666666666667\n"
     ]
    }
   ],
   "source": [
    "# SVM Model implemented using scikit-learn\n",
    "svm_tfidf = LinearSVC(C=0.1)\n",
    "svm_tfidf.fit(X_TRAIN_TFIDF,y_train_tfidf)\n",
    "svm_pred_tfidf = svm_tfidf.predict(X_TEST_TFIDF)\n",
    "print(\"Accuracy score for SVC with TF-IDF Embeddings : \",accuracy_score(y_test_tfidf,svm_pred_tfidf)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1657e7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d4174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d5e20f",
   "metadata": {},
   "source": [
    "#### Converting mean embeddings of reviews arrays to train Perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c77774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[] for i in range(300)]\n",
    "for vec in data['mean_embeddings']:\n",
    "    for i in range(300):\n",
    "        vectors[i].append(vec[i])\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9bfbc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame()\n",
    "for i in range(300):\n",
    "    X['vec'+str(i)] = vectors[i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1f31b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec0</th>\n",
       "      <th>vec1</th>\n",
       "      <th>vec2</th>\n",
       "      <th>vec3</th>\n",
       "      <th>vec4</th>\n",
       "      <th>vec5</th>\n",
       "      <th>vec6</th>\n",
       "      <th>vec7</th>\n",
       "      <th>vec8</th>\n",
       "      <th>vec9</th>\n",
       "      <th>...</th>\n",
       "      <th>vec290</th>\n",
       "      <th>vec291</th>\n",
       "      <th>vec292</th>\n",
       "      <th>vec293</th>\n",
       "      <th>vec294</th>\n",
       "      <th>vec295</th>\n",
       "      <th>vec296</th>\n",
       "      <th>vec297</th>\n",
       "      <th>vec298</th>\n",
       "      <th>vec299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027973</td>\n",
       "      <td>0.036503</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.069017</td>\n",
       "      <td>-0.070432</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.036540</td>\n",
       "      <td>-0.101999</td>\n",
       "      <td>0.035219</td>\n",
       "      <td>0.061851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016587</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>-0.054076</td>\n",
       "      <td>0.013828</td>\n",
       "      <td>-0.011767</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>-0.012971</td>\n",
       "      <td>0.024813</td>\n",
       "      <td>-0.005301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000359</td>\n",
       "      <td>0.039180</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>0.095080</td>\n",
       "      <td>-0.076810</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>-0.063374</td>\n",
       "      <td>0.057230</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041360</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>-0.052340</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>-0.024643</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>0.035273</td>\n",
       "      <td>-0.008781</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>-0.021927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056091</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.050641</td>\n",
       "      <td>0.124902</td>\n",
       "      <td>-0.080347</td>\n",
       "      <td>0.012112</td>\n",
       "      <td>0.053739</td>\n",
       "      <td>-0.049774</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023947</td>\n",
       "      <td>0.050580</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>-0.017410</td>\n",
       "      <td>-0.007632</td>\n",
       "      <td>0.007876</td>\n",
       "      <td>0.053277</td>\n",
       "      <td>-0.024380</td>\n",
       "      <td>0.042681</td>\n",
       "      <td>-0.019931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039604</td>\n",
       "      <td>-0.048157</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>-0.070984</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>0.100708</td>\n",
       "      <td>-0.015015</td>\n",
       "      <td>0.053650</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134033</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>-0.020287</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.060495</td>\n",
       "      <td>0.067688</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.027206</td>\n",
       "      <td>0.062683</td>\n",
       "      <td>-0.066040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048212</td>\n",
       "      <td>0.030977</td>\n",
       "      <td>0.036350</td>\n",
       "      <td>0.083562</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>-0.022835</td>\n",
       "      <td>0.037558</td>\n",
       "      <td>-0.085736</td>\n",
       "      <td>0.032672</td>\n",
       "      <td>0.051963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020280</td>\n",
       "      <td>0.033158</td>\n",
       "      <td>-0.076561</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>-0.022048</td>\n",
       "      <td>-0.010311</td>\n",
       "      <td>0.025846</td>\n",
       "      <td>-0.031428</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>-0.040568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59994</th>\n",
       "      <td>0.040577</td>\n",
       "      <td>0.044389</td>\n",
       "      <td>-0.006126</td>\n",
       "      <td>0.055520</td>\n",
       "      <td>-0.042503</td>\n",
       "      <td>0.019842</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>-0.023837</td>\n",
       "      <td>0.097712</td>\n",
       "      <td>0.085283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>0.037703</td>\n",
       "      <td>-0.046402</td>\n",
       "      <td>0.041681</td>\n",
       "      <td>-0.031547</td>\n",
       "      <td>-0.019509</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>-0.093563</td>\n",
       "      <td>-0.032167</td>\n",
       "      <td>-0.044101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.038660</td>\n",
       "      <td>0.028087</td>\n",
       "      <td>0.057663</td>\n",
       "      <td>-0.065738</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.029206</td>\n",
       "      <td>-0.069214</td>\n",
       "      <td>0.053897</td>\n",
       "      <td>0.074296</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>0.042465</td>\n",
       "      <td>-0.054823</td>\n",
       "      <td>0.022469</td>\n",
       "      <td>-0.025906</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>-0.009488</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>-0.014272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>0.016621</td>\n",
       "      <td>0.043959</td>\n",
       "      <td>-0.059263</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.048660</td>\n",
       "      <td>-0.075557</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.043687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059424</td>\n",
       "      <td>0.022999</td>\n",
       "      <td>-0.070801</td>\n",
       "      <td>-0.005056</td>\n",
       "      <td>-0.012317</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>-0.007563</td>\n",
       "      <td>-0.001718</td>\n",
       "      <td>0.056476</td>\n",
       "      <td>-0.001414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>0.017404</td>\n",
       "      <td>0.045056</td>\n",
       "      <td>0.018658</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>-0.081627</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>-0.069254</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060959</td>\n",
       "      <td>0.076282</td>\n",
       "      <td>-0.055921</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>-0.016364</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-0.008748</td>\n",
       "      <td>-0.007415</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>-0.022205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>0.057936</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.070340</td>\n",
       "      <td>-0.027605</td>\n",
       "      <td>-0.012484</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>-0.078717</td>\n",
       "      <td>0.074017</td>\n",
       "      <td>0.039479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057066</td>\n",
       "      <td>0.047101</td>\n",
       "      <td>-0.095002</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>-0.034628</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>-0.004551</td>\n",
       "      <td>-0.054393</td>\n",
       "      <td>0.037859</td>\n",
       "      <td>-0.066401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59999 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           vec0      vec1      vec2      vec3      vec4      vec5      vec6  \\\n",
       "0      0.027973  0.036503  0.031750  0.069017 -0.070432  0.001462  0.036540   \n",
       "1     -0.000359  0.039180 -0.000855  0.095080 -0.076810  0.001209  0.037842   \n",
       "2      0.056091  0.034900  0.050641  0.124902 -0.080347  0.012112  0.053739   \n",
       "3      0.039604 -0.048157  0.130859  0.023956 -0.070984  0.068390  0.100708   \n",
       "4      0.048212  0.030977  0.036350  0.083562 -0.033732 -0.022835  0.037558   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "59994  0.040577  0.044389 -0.006126  0.055520 -0.042503  0.019842  0.050526   \n",
       "59995  0.011458  0.038660  0.028087  0.057663 -0.065738  0.006164  0.029206   \n",
       "59996  0.014291  0.010903  0.016621  0.043959 -0.059263  0.021224  0.048660   \n",
       "59997  0.017404  0.045056  0.018658  0.084900 -0.081627  0.025398  0.028895   \n",
       "59998  0.057936  0.014143  0.027200  0.070340 -0.027605 -0.012484  0.024951   \n",
       "\n",
       "           vec7      vec8      vec9  ...    vec290    vec291    vec292  \\\n",
       "0     -0.101999  0.035219  0.061851  ... -0.016587  0.027404 -0.054076   \n",
       "1     -0.063374  0.057230  0.008774  ... -0.041360  0.059261 -0.052340   \n",
       "2     -0.049774  0.024353  0.023747  ...  0.023947  0.050580 -0.035054   \n",
       "3     -0.015015  0.053650  0.050262  ... -0.134033  0.011353 -0.020287   \n",
       "4     -0.085736  0.032672  0.051963  ... -0.020280  0.033158 -0.076561   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "59994 -0.023837  0.097712  0.085283  ... -0.036621  0.037703 -0.046402   \n",
       "59995 -0.069214  0.053897  0.074296  ... -0.053467  0.042465 -0.054823   \n",
       "59996 -0.075557  0.020397  0.043687  ... -0.059424  0.022999 -0.070801   \n",
       "59997 -0.069254  0.030235  0.071003  ... -0.060959  0.076282 -0.055921   \n",
       "59998 -0.078717  0.074017  0.039479  ... -0.057066  0.047101 -0.095002   \n",
       "\n",
       "         vec293    vec294    vec295    vec296    vec297    vec298    vec299  \n",
       "0      0.013828 -0.011767  0.021082  0.013434 -0.012971  0.024813 -0.005301  \n",
       "1      0.003616 -0.024643  0.008549  0.035273 -0.008781  0.008684 -0.021927  \n",
       "2     -0.017410 -0.007632  0.007876  0.053277 -0.024380  0.042681 -0.019931  \n",
       "3      0.056641  0.060495  0.067688 -0.008636 -0.027206  0.062683 -0.066040  \n",
       "4      0.011807 -0.022048 -0.010311  0.025846 -0.031428  0.014893 -0.040568  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "59994  0.041681 -0.031547 -0.019509  0.025400 -0.093563 -0.032167 -0.044101  \n",
       "59995  0.022469 -0.025906  0.007971  0.007524 -0.009488 -0.000210 -0.014272  \n",
       "59996 -0.005056 -0.012317  0.009363 -0.007563 -0.001718  0.056476 -0.001414  \n",
       "59997  0.005642 -0.016364 -0.000544 -0.008748 -0.007415  0.020001 -0.022205  \n",
       "59998  0.002133 -0.034628  0.000784 -0.004551 -0.054393  0.037859 -0.066401  \n",
       "\n",
       "[59999 rows x 300 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7330b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "044165b5",
   "metadata": {},
   "source": [
    "#### Splitting data into X_train, X_test, y_train and y_test using train_test_split method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8190d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, data['rating'], stratify=data['rating'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ceec76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17a7f72",
   "metadata": {},
   "source": [
    "#### Perceptron model with pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aae566b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prc = Perceptron(random_state=5)\n",
    "prc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6abd1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_pred = prc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15ee5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Perceptron model with Pretrained Word Embeddings :  63.4\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score for Perceptron model with Pretrained Word Embeddings : \",accuracy_score(y_test,prc_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "888faf02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC(C=0.1)\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71f7bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "151b2d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVC model with Pretrained Word Embeddings :  65.9\n"
     ]
    }
   ],
   "source": [
    "svc_precision = precision_score(y_test,svc_pred,average=None)\n",
    "print(\"Accuracy score for SVC model with Pretrained Word Embeddings : \",accuracy_score(y_test,svc_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92266d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8ec229",
   "metadata": {},
   "source": [
    "<b><i>What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?</i></b>\n",
    "\n",
    "<i>Ans. </i> On the basis of the accuracy score, it can be concluded that models with TF-IDF embeddings are equivalent or better than those with pretrained Word2Vec. As it can be seen that accuracy for Perceptron with TFIDF is 62.4% almost comparable to 63.4% with Pretrained Embeddings. While accuracy for SVC with TFIDF is 71.2 compared to 65.9% with Pretrained Embeddings where the difference is considerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1249a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43935f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b51db24",
   "metadata": {},
   "source": [
    "# Q - 4. FeedForward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5aca7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have trained all the models on GPU available in my PC.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "75e36d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88af5e",
   "metadata": {},
   "source": [
    "## 4-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea755b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepareData function will preprocess the data to convert it to torch loader\n",
    "def prepareData(x,y, batch_size):\n",
    "    data_list = []\n",
    "    label_list = np.array(y)\n",
    "    for i in range(len(x)):\n",
    "        data_list.append((x.iloc[i,:].values, label_list[i]-1))\n",
    "    \n",
    "        \n",
    "    loader = torch.utils.data.DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "train_loader = prepareData(X_train, y_train,32)\n",
    "test_loader = prepareData(X_test, y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "673cdb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - 2\n",
    "# Class Net -> inherits Module class of nn and implements multilayer perceptron model with 2 hidden layers 100 and 10 respectively.\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_1,hidden_2):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_1).to(device),\n",
    "            # I have tried different activation functions like ReLu and tanh but LeakyRelu is giving best result. \n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(hidden_1, hidden_2).to(device),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(hidden_2, 3).to(device),\n",
    "            # I have used dropout layer to make the model generalize well.\n",
    "            torch.nn.Dropout(0.35)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2ca8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(300,100,10)\n",
    "# loading model on GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d336109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used CrossEntropy as loss function and Adam optimizer with learning rate 0.001\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5b62000c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=10, out_features=3, bias=True)\n",
       "    (5): Dropout(p=0.35, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0406160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch --> 1 : 0.917518109658745\n",
      "Epoch --> 2 : 0.8568418967619288\n",
      "Epoch --> 3 : 0.8420849370358872\n",
      "Epoch --> 4 : 0.8321219237884553\n",
      "Epoch --> 5 : 0.8258142589593908\n",
      "Epoch --> 6 : 0.8245183939513953\n",
      "Epoch --> 7 : 0.8155691596666409\n",
      "Epoch --> 8 : 0.8090611771843419\n",
      "Epoch --> 9 : 0.8011510197342767\n",
      "Epoch --> 10 : 0.7961904746673637\n",
      "Epoch --> 11 : 0.7954849854501904\n",
      "Epoch --> 12 : 0.7881521744880481\n",
      "Epoch --> 13 : 0.7880292705611469\n",
      "Epoch --> 14 : 0.7800116352050085\n",
      "Epoch --> 15 : 0.7784965042298122\n",
      "Epoch --> 16 : 0.7753646418627045\n",
      "Epoch --> 17 : 0.7703143528989038\n",
      "Epoch --> 18 : 0.7676170001107158\n",
      "Epoch --> 19 : 0.7621021360384305\n",
      "Epoch --> 20 : 0.7581244316366121\n",
      "Epoch --> 21 : 0.7588211240556136\n",
      "Epoch --> 22 : 0.7521176020782037\n",
      "Epoch --> 23 : 0.7471179353681444\n",
      "Epoch --> 24 : 0.7442011698258092\n",
      "Epoch --> 25 : 0.7404425133491332\n",
      "Epoch --> 26 : 0.7418690804228261\n",
      "Epoch --> 27 : 0.7356238405034776\n",
      "Epoch --> 28 : 0.7313561851378418\n",
      "Epoch --> 29 : 0.7265083836628479\n",
      "Epoch --> 30 : 0.7276578983946833\n"
     ]
    }
   ],
   "source": [
    "# I have trained MLP for 30 epochs and at each epoch I have printed the respective loss value\n",
    "valid_loss_min = np.Inf \n",
    "n_epochs = 30\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (train_data, target) in enumerate(train_loader):\n",
    "        train_data = train_data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_data.float())\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*train_data.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print(\"Epoch --> \"+str(epoch+1)+\" :\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d863381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure model's accuracy on test loader or any loader.\n",
    "def evalAccuracy(testModel, loader):\n",
    "    testModel.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = testModel(inputs.float())\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "#     print(\"Accuracy : \", correct/total)\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "834732a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Multilayer Perceptron for Task-A is :  67.11666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy for Multilayer Perceptron for Task-A is : \",evalAccuracy(model,test_loader)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc83443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b0645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62ea7133",
   "metadata": {},
   "source": [
    "### 4-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c5b5edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting first 10 embedding vectors for each review if length is less than 10 words the appending Zeros\n",
    "ten_embeddings = []\n",
    "for embd in data[\"pretrained_embeddings_reviews\"]:\n",
    "    temp_embd=[]\n",
    "    for i in range(min(len(embd),10)):\n",
    "        temp_embd.extend(embd[i])\n",
    "    if len(temp_embd)<3000:\n",
    "        for i in range(3000-len(temp_embd)):\n",
    "            temp_embd.extend([0.0])\n",
    "    ten_embeddings.append(temp_embd)\n",
    "data[\"top_ten_vectors\"] = ten_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75a95a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [-0.011779785, -0.04736328, 0.044677734, 0.063...\n",
       "1        [0.084472656, -0.0003528595, 0.053222656, 0.09...\n",
       "2        [0.064453125, 0.036132812, 0.03857422, 0.09472...\n",
       "3        [0.0070495605, -0.07324219, 0.171875, 0.022583...\n",
       "4        [0.20019531, 0.15429688, 0.103027344, 0.008666...\n",
       "                               ...                        \n",
       "59995    [0.048828125, 0.16699219, 0.16894531, 0.087402...\n",
       "59996    [0.109375, 0.140625, -0.03173828, 0.16601562, ...\n",
       "59997    [-0.22558594, -0.01953125, 0.09082031, 0.23730...\n",
       "59998    [-0.22558594, -0.01953125, 0.09082031, 0.23730...\n",
       "59999    [-0.22558594, -0.01953125, 0.09082031, 0.23730...\n",
       "Name: top_ten_vectors, Length: 59999, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"top_ten_vectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9aa91e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data into training and testing data using train_test_split method\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(data[\"top_ten_vectors\"],data[\"rating\"],random_state=0,stratify=data[\"rating\"],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3d241b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embd_ratings_train = pd.DataFrame(data={\"embeddings_train\":X_train_b,\"Ratings_train\":y_train_b})\n",
    "df_embd_ratings_test = pd.DataFrame(data={\"embeddings_test\":X_test_b,\"Ratings_test\":y_test_b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a2ee8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embd_ratings_train.reset_index(inplace=True,drop=True)\n",
    "df_embd_ratings_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f62ad70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings_train</th>\n",
       "      <th>Ratings_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.115234375, -0.15527344, 0.20019531, 0.2968...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.22558594, -0.01953125, 0.09082031, 0.23730...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.084472656, -0.0003528595, 0.053222656, 0.09...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.13964844, -0.03466797, -0.053710938, 0.179...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.22558594, -0.01953125, 0.09082031, 0.23730...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47994</th>\n",
       "      <td>[-0.016235352, 0.09423828, 0.091796875, 0.1196...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47995</th>\n",
       "      <td>[0.08496094, -0.095214844, 0.119140625, 0.1118...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47996</th>\n",
       "      <td>[0.109375, 0.140625, -0.03173828, 0.16601562, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47997</th>\n",
       "      <td>[0.16894531, 0.063964844, -0.084472656, 0.1738...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47998</th>\n",
       "      <td>[0.123046875, 0.012817383, 0.01940918, 0.23046...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        embeddings_train  Ratings_train\n",
       "0      [-0.115234375, -0.15527344, 0.20019531, 0.2968...              2\n",
       "1      [-0.22558594, -0.01953125, 0.09082031, 0.23730...              3\n",
       "2      [0.084472656, -0.0003528595, 0.053222656, 0.09...              1\n",
       "3      [-0.13964844, -0.03466797, -0.053710938, 0.179...              2\n",
       "4      [-0.22558594, -0.01953125, 0.09082031, 0.23730...              1\n",
       "...                                                  ...            ...\n",
       "47994  [-0.016235352, 0.09423828, 0.091796875, 0.1196...              3\n",
       "47995  [0.08496094, -0.095214844, 0.119140625, 0.1118...              2\n",
       "47996  [0.109375, 0.140625, -0.03173828, 0.16601562, ...              2\n",
       "47997  [0.16894531, 0.063964844, -0.084472656, 0.1738...              2\n",
       "47998  [0.123046875, 0.012817383, 0.01940918, 0.23046...              1\n",
       "\n",
       "[47999 rows x 2 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embd_ratings_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84f54273",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_b = []\n",
    "labelX_b = np.array(df_embd_ratings_train.iloc[:,1])\n",
    "for i in range(len(X_train_b)):\n",
    "    trainX_b.append((np.array(df_embd_ratings_train.iloc[i,0]),labelX_b[i]-1))\n",
    "    \n",
    "train_loader_b = torch.utils.data.DataLoader(trainX_b,batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3d4b6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX_b = []\n",
    "label_X_test_b = np.array(df_embd_ratings_test.iloc[:,1])\n",
    "for i in range(len(X_test_b)):\n",
    "    testX_b.append((np.array(df_embd_ratings_test.iloc[i,0]),label_X_test_b[i]-1))\n",
    "    \n",
    "test_loader_b = torch.utils.data.DataLoader(testX_b,batch_size=1,shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5e27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1b462e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class NetB -> inherits Module class from nn and implements MLP for task 4-B\n",
    "class NetB(torch.nn.Module):\n",
    "    def __init__(self,input_size,hidden_1,hidden_2):\n",
    "        super(NetB, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_1).to(device),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            # Here I have used BatchNorm1d which basically takes 1d input and normalizes it. Adding the layer was proved to be fruitful for me\n",
    "            torch.nn.BatchNorm1d(hidden_1),\n",
    "            torch.nn.Linear(hidden_1, hidden_2).to(device),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.BatchNorm1d(hidden_2),\n",
    "            torch.nn.Linear(hidden_2, 3).to(device),\n",
    "            torch.nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4e36997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating instance of the NetB\n",
    "mlp_model_b = NetB(3000,100,10)\n",
    "mlp_model_b = mlp_model_b.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf5caa7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetB(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=3000, out_features=100, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Dropout(p=0.25, inplace=False)\n",
       "    (7): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Linear(in_features=10, out_features=3, bias=True)\n",
       "    (9): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aa2925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used CrossEntropy as loss function and Adam optimizer with learning rate 0.001\n",
    "loss_fn_b = torch.nn.CrossEntropyLoss()\n",
    "optimizer_b = torch.optim.Adam(mlp_model_b.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3662b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch --> 1 : 1.0375636212316492\n",
      "Epoch --> 2 : 0.9675931537962722\n",
      "Epoch --> 3 : 0.9442634624703113\n",
      "Epoch --> 4 : 0.9301355587320752\n",
      "Epoch --> 5 : 0.9162100125691323\n",
      "Epoch --> 6 : 0.9010592327122589\n",
      "Epoch --> 7 : 0.8850530789589847\n",
      "Epoch --> 8 : 0.8775866482416246\n",
      "Epoch --> 9 : 0.8579570799584801\n",
      "Epoch --> 10 : 0.8478754551667129\n",
      "Epoch --> 11 : 0.8295534680817495\n",
      "Epoch --> 12 : 0.8226434250976069\n",
      "Epoch --> 13 : 0.8085739037966181\n",
      "Epoch --> 14 : 0.7995906678678423\n",
      "Epoch --> 15 : 0.786379399931742\n",
      "Epoch --> 16 : 0.7707403294878549\n",
      "Epoch --> 17 : 0.7687466458821387\n",
      "Epoch --> 18 : 0.7593901240616288\n",
      "Epoch --> 19 : 0.745616045920123\n",
      "Epoch --> 20 : 0.7377580535936118\n",
      "Epoch --> 21 : 0.7274228677036548\n",
      "Epoch --> 22 : 0.7134033683406882\n",
      "Epoch --> 23 : 0.7133043493543353\n",
      "Epoch --> 24 : 0.7025346198579481\n",
      "Epoch --> 25 : 0.6963275897755976\n",
      "Epoch --> 26 : 0.684203249346095\n",
      "Epoch --> 27 : 0.6835889105794429\n",
      "Epoch --> 28 : 0.6734783782676155\n",
      "Epoch --> 29 : 0.6601606641350399\n",
      "Epoch --> 30 : 0.6571511364871778\n",
      "Epoch --> 31 : 0.6549592974205862\n",
      "Epoch --> 32 : 0.6480018911804963\n",
      "Epoch --> 33 : 0.6376820499716844\n",
      "Epoch --> 34 : 0.6297598756209779\n",
      "Epoch --> 35 : 0.6272112084812451\n",
      "Epoch --> 36 : 0.6233612171619961\n"
     ]
    }
   ],
   "source": [
    "# I have trained for longer period and found 36 to be a good value for epochs\n",
    "valid_loss_min = np.Inf \n",
    "n_epochs = 36\n",
    "for epoch in range(n_epochs):\n",
    "    mlp_model_b.train()\n",
    "    train_loss_b = 0.0\n",
    "    \n",
    "    for i, (train_data, target) in enumerate(train_loader_b):\n",
    "        train_data = train_data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer_b.zero_grad()\n",
    "        output = mlp_model_b(train_data.float())\n",
    "        loss = loss_fn_b(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_b.step()\n",
    "        train_loss_b += loss.item()*train_data.size(0)\n",
    "\n",
    "    mlp_model_b.eval()\n",
    "    train_loss_b = train_loss_b/len(train_loader_b.dataset)\n",
    "    print(\"Epoch --> \"+str(epoch+1)+\" :\", train_loss_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e8a37ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MLP for task 4-b :  56.43333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for MLP for task 4-b : \",evalAccuracy(mlp_model_b,test_loader_b)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e25c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "649fe379",
   "metadata": {},
   "source": [
    "<b><i>What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section?</i></b>\n",
    "\n",
    "<i>Ans. </i>The accuracies of Multilayer Perceptron for part a and b are 67.11% and 56.43% respectively. The accuracy for MLP of part-a is more than accuracy of Perceptron and SVC models with pretrained word embeddings. On the other hand, model trained as of part-b is not doing better comapred to Simple Models. The statistics lead us to model of part 4-a as best so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b9228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc0531a2",
   "metadata": {},
   "source": [
    "# Q - 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e4822",
   "metadata": {},
   "source": [
    "#### Preparing training data and validation data for RNN, GRU and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e686d40",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35cdfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting embeddings for 20 words in review. If length is less than 20 than padding with zeros for training data.\n",
    "embedding_20 = []\n",
    "\n",
    "for embd in data[\"pretrained_embeddings_reviews\"]:\n",
    "    temp_embd=[]\n",
    "    for i in range(min(len(embd),20)):\n",
    "        temp_embd.append(embd[i])\n",
    "    if len(temp_embd)<20:\n",
    "        for j in range(20-len(temp_embd)):\n",
    "            temp_embd.append(np.zeros(300))\n",
    "    embedding_20.append(temp_embd)\n",
    "data[\"top_20_embeddings\"] = embedding_20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d9e5b",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9c088498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting embeddings for 20 words in review. If length is less than 20 than padding with zeros for validation data\n",
    "embedding_20_val = []\n",
    "\n",
    "for embd in final_df_val[\"pretrained_embeddings_reviews\"]:\n",
    "    temp_embd=[]\n",
    "    for i in range(min(len(embd),20)):\n",
    "        temp_embd.append(embd[i])\n",
    "    if len(temp_embd)<20:\n",
    "        for j in range(20-len(temp_embd)):\n",
    "            temp_embd.append(np.zeros(300))\n",
    "    embedding_20_val.append(temp_embd)\n",
    "final_df_val[\"top_20_embeddings\"] = embedding_20_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4662969f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.109375, 0.140625, -0.03173828, 0.16601562,...\n",
       "1        [[-0.055908203, 0.11767578, 0.2109375, 0.00836...\n",
       "2        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "3        [[-0.22558594, -0.01953125, 0.09082031, 0.2373...\n",
       "4        [[0.08496094, -0.095214844, 0.119140625, 0.111...\n",
       "                               ...                        \n",
       "11995    [[-0.22558594, -0.01953125, 0.09082031, 0.2373...\n",
       "11996    [[0.109375, 0.140625, -0.03173828, 0.16601562,...\n",
       "11997    [[-0.12695312, 0.021972656, 0.28710938, 0.1533...\n",
       "11998    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "11999    [[-0.12695312, 0.021972656, 0.28710938, 0.1533...\n",
       "Name: top_20_embeddings, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_val[\"top_20_embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78e5426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting embeddings of first 20 words into train and test\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(data[\"top_20_embeddings\"],data[\"rating\"],random_state=0,stratify=data[\"rating\"],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f7aff374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted them to dataframe for faster computation when loading in torch loader\n",
    "df_seq_embd_ratings_train = pd.DataFrame(data={\"embeddings_train\":X_train_seq,\"Ratings_train\":y_train_seq})\n",
    "df_seq_embd_ratings_test = pd.DataFrame(data={\"embeddings_test\":X_test_seq,\"Ratings_test\":y_test_seq})\n",
    "df_seq_embd_ratings_val = pd.DataFrame(data={\"embeddings_test\":final_df_val[\"top_20_embeddings\"].values,\"Ratings_test\":final_df_val[\"rating\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "34716802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seq_embd_ratings_train.reset_index(inplace=True,drop=True)\n",
    "df_seq_embd_ratings_test.reset_index(drop=True,inplace=True)\n",
    "df_seq_embd_ratings_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e4117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "78be1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Loader\n",
    "trainX_seq = []\n",
    "labelX_seq = np.array(df_seq_embd_ratings_train.iloc[:,1])\n",
    "for i in range(len(X_train_seq)):\n",
    "    trainX_seq.append((np.array(df_seq_embd_ratings_train.iloc[i,0]),labelX_seq[i]-1))\n",
    "    \n",
    "train_loader_seq = torch.utils.data.DataLoader(trainX_seq,batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8c491f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Validation Loader\n",
    "trainX_seq_val = []\n",
    "labelX_seq_val = np.array(df_seq_embd_ratings_val.iloc[:,1])\n",
    "for i in range(len(final_df_val)):\n",
    "    trainX_seq_val.append((np.array(df_seq_embd_ratings_val.iloc[i,0]),labelX_seq_val[i]-1))\n",
    "    \n",
    "val_loader_seq = torch.utils.data.DataLoader(trainX_seq_val,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "25b3c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preaparing Testing Loader\n",
    "testX_seq = []\n",
    "label_X_test_seq = np.array(df_seq_embd_ratings_test.iloc[:,1])\n",
    "for i in range(len(X_test_seq)):\n",
    "    testX_seq.append((np.array(df_seq_embd_ratings_test.iloc[i,0]),label_X_test_seq[i]-1))\n",
    "    \n",
    "test_loader_seq = torch.utils.data.DataLoader(testX_seq,batch_size=1,shuffle=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f75ae7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalAccuracySeq() -> returns loss and accuracy of a give model on given torch loader\n",
    "def evalAccuracySeq(testModel, loader):\n",
    "    testModel.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = testModel(inputs.float())\n",
    "            loss = loss_fn_b(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss/len(loader.dataset), correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133c2a7",
   "metadata": {},
   "source": [
    "## A) RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "01a4a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference -3\n",
    "# class RNN -> inherits Module class and implements RNN model with hidden state size 20\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=20,num_layers = 1,num_classes=3):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # Default nonlinearity is tanh but I have kept relu as it is giving better accuracy. \n",
    "        # I have also added dropout=0.5 which seems give better performance\n",
    "        self.rnn = torch.nn.RNN(input_size,hidden_size,num_layers,batch_first=True,dropout=0.5,nonlinearity=\"relu\")\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # h0->hidden layer\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out,_ = self.rnn(x,h0)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a36f98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c203f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used CrossEntropy as loss function and Adam optimizer with learning rate 0.001\n",
    "rnn_model = rnn_model.to(device)\n",
    "loss_fn_seq = torch.nn.CrossEntropyLoss()\n",
    "optimizer_seq = torch.optim.Adam(rnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e169fa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss :  0.912119974612336\n",
      "Epoch --> 1 : 1.024930572573842\n",
      "Validation Loss :  0.8840110482301874\n",
      "Epoch --> 2 : 0.8829115798998436\n",
      "Validation Loss :  0.8583522320714546\n",
      "Epoch --> 3 : 0.8534464557152639\n",
      "Validation Loss :  0.8514065974607365\n",
      "Epoch --> 4 : 0.8348199229130743\n",
      "Validation Loss :  0.8327555922352476\n",
      "Epoch --> 5 : 0.8209328012515208\n",
      "Validation Loss :  0.8670534083273136\n",
      "Epoch --> 6 : 0.80963838869846\n",
      "Validation Loss :  0.821337249206553\n",
      "Epoch --> 7 : 0.801229065233336\n",
      "Validation Loss :  0.8288414204498598\n",
      "Epoch --> 8 : 0.7920327537588637\n",
      "Validation Loss :  0.8086143044211155\n",
      "Epoch --> 9 : 0.7873898357517404\n",
      "Validation Loss :  0.8140081022060476\n",
      "Epoch --> 10 : 0.7811669717828652\n",
      "Validation Loss :  0.8011406514071908\n",
      "Epoch --> 11 : 0.775950805583684\n",
      "Validation Loss :  0.7997821332118425\n",
      "Epoch --> 12 : 0.769126855565095\n",
      "Validation Loss :  0.8073431345584169\n",
      "Epoch --> 13 : 0.7669704111618967\n",
      "Validation Loss :  0.8082664725364507\n",
      "Epoch --> 14 : 0.7618929618259139\n",
      "Validation Loss :  0.7944464061671728\n",
      "Epoch --> 15 : 0.7610737944511591\n",
      "Validation Loss :  0.8316101614856787\n",
      "Epoch --> 16 : 0.7539335992682037\n",
      "Validation Loss :  0.7949570479562014\n",
      "Epoch --> 17 : 0.7527433570758678\n",
      "Validation Loss :  0.8068976753890902\n",
      "Epoch --> 18 : 0.7489280111715345\n",
      "Validation Loss :  0.7965284801718423\n",
      "Epoch --> 19 : 0.7480764484519266\n",
      "Validation Loss :  0.8003228601131859\n",
      "Epoch --> 20 : 0.7448852832184382\n",
      "Validation Loss :  0.8059805760409703\n",
      "Epoch --> 21 : 0.7419721984600519\n",
      "Validation Loss :  0.7989793733621486\n",
      "Epoch --> 22 : 0.7379297347473014\n",
      "Validation Loss :  0.7920900129346701\n",
      "Epoch --> 23 : 0.7402250002470088\n",
      "Validation Loss :  0.793125555472199\n",
      "Epoch --> 24 : 0.7362106114221728\n",
      "Validation Loss :  0.7988260890480745\n",
      "Epoch --> 25 : 0.7341260145899132\n",
      "Validation Loss :  0.8022769339020266\n",
      "Epoch --> 26 : 0.7317475160225404\n",
      "Validation Loss :  0.8101300812755492\n",
      "Epoch --> 27 : 0.7291878377296753\n",
      "Validation Loss :  0.7944867914390342\n",
      "Epoch --> 28 : 0.7274726466595222\n",
      "Validation Loss :  0.8113429220474855\n",
      "Epoch --> 29 : 0.7272760035080245\n",
      "Validation Loss :  0.8184510934572393\n",
      "Epoch --> 30 : 0.7234726738263156\n",
      "Validation Loss :  0.7954166719597197\n",
      "Epoch --> 31 : 0.7229187821107084\n",
      "Validation Loss :  0.8005852148699423\n",
      "Epoch --> 32 : 0.7217583358803153\n",
      "Validation Loss :  0.8079685193101405\n",
      "Epoch --> 33 : 0.7261075617974047\n",
      "Validation Loss :  0.7961922488490404\n",
      "Epoch --> 34 : 0.7246152985634666\n",
      "Validation Loss :  0.8095404806322488\n",
      "Epoch --> 35 : 0.7202977265672671\n",
      "Validation Loss :  0.7992606390170295\n",
      "Epoch --> 36 : 0.7161319970726959\n",
      "Validation Loss :  0.8016431342152461\n",
      "Epoch --> 37 : 0.7150224668785577\n",
      "Validation Loss :  0.8023346285665465\n",
      "Epoch --> 38 : 0.7162019091250115\n",
      "Validation Loss :  0.8137389407481552\n",
      "Epoch --> 39 : 0.714105490524398\n",
      "Validation Loss :  0.8079768286606436\n",
      "Epoch --> 40 : 0.7145796009157024\n",
      "Validation Loss :  0.8077537073146448\n",
      "Epoch --> 41 : 0.7103274406789529\n",
      "Validation Loss :  0.8073979101674779\n",
      "Epoch --> 42 : 0.7106081614005257\n",
      "Validation Loss :  0.8074600832321147\n",
      "Epoch --> 43 : 0.7102850249673991\n",
      "Validation Loss :  0.8134710043006911\n",
      "Epoch --> 44 : 0.7062750402973424\n",
      "Validation Loss :  0.8162958212723961\n",
      "Epoch --> 45 : 0.7055845024344867\n",
      "Validation Loss :  0.8127965447189346\n",
      "Epoch --> 46 : 0.704162996019775\n",
      "Validation Loss :  0.8228490963920007\n",
      "Epoch --> 47 : 0.705629743805314\n",
      "Validation Loss :  0.8016590155557282\n",
      "Epoch --> 48 : 0.7024262284904215\n",
      "Validation Loss :  0.8154712539865537\n",
      "Epoch --> 49 : 0.7013454380568178\n",
      "Validation Loss :  0.8313960676351299\n",
      "Epoch --> 50 : 0.70213546960234\n"
     ]
    }
   ],
   "source": [
    "# I have trained RNN model for 50 epochs. At each epoch I am testing my model on unseen validation data and keeping the model \n",
    "# with lowest loss on validation data.\n",
    "valid_loss_min = np.Inf \n",
    "n_epochs = 50\n",
    "best_rnn_model = None\n",
    "best_model_epoch = None\n",
    "for epoch in range(n_epochs):\n",
    "    rnn_model.train()\n",
    "    train_loss_seq = 0.0\n",
    "    \n",
    "    for i, (train_data, target) in enumerate(train_loader_seq):\n",
    "        train_data = train_data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer_seq.zero_grad()\n",
    "        output = rnn_model(train_data.float())\n",
    "        loss = loss_fn_seq(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_seq.step()\n",
    "        train_loss_seq += loss.item()*train_data.size(0)\n",
    "\n",
    "    rnn_model.eval()\n",
    "    val_loss, val_acc = evalAccuracySeq(rnn_model,val_loader_seq)\n",
    "    print(\"Validation Loss : \", val_loss)\n",
    "    if valid_loss_min>val_loss:\n",
    "        valid_loss_min = val_loss\n",
    "        best_rnn_model = copy.deepcopy(rnn_model)\n",
    "        best_model_epoch = epoch+1\n",
    "        \n",
    "    train_loss_seq = train_loss_seq/len(train_loader_seq.dataset)\n",
    "    print(\"Epoch --> \"+str(epoch+1)+\" :\", train_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "994fcf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RNN Model is :  64.075\n"
     ]
    }
   ],
   "source": [
    "# I am comparing the 2 models. One with lowest validation loss and one after training for given number of epochs\n",
    "rnn_accuracy = max(evalAccuracy(rnn_model,test_loader_seq),evalAccuracy(best_rnn_model,test_loader_seq))\n",
    "print(\"Accuracy for RNN Model is : \", rnn_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65726a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b62cf1d2",
   "metadata": {},
   "source": [
    "<b><i>What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network model?</i></b>\n",
    "\n",
    "The accuracy for RNN model is 64.075% which is better than feedforward network of part 4-b but not that good compared to part 4-a. So compared to simple RNN, simple MLP is better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3afe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09afe9d",
   "metadata": {},
   "source": [
    "## B) GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5c26611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - 4\n",
    "# class GRU -> inherits Module class and implements GRU model using torch.nn.GRU\n",
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=20,num_layers = 1,num_classes=3):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = torch.nn.GRU(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_classes,bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out,_ = self.gru(x,h0)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0d861a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = GRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e9921712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used CrossEntropy as loss function and Adam optimizer with learning rate 0.001\n",
    "gru_model = gru_model.to(device)\n",
    "loss_fn_gru = torch.nn.CrossEntropyLoss()\n",
    "optimizer_gru = torch.optim.Adam(gru_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "000c2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss :  0.8588361076399063\n",
      "Epoch --> 1 : 0.9426594571202082\n",
      "Validation Loss :  0.7983265329046796\n",
      "Epoch --> 2 : 0.8084148433959053\n",
      "Validation Loss :  0.7811873122374527\n",
      "Epoch --> 3 : 0.7799193326782521\n",
      "Validation Loss :  0.7651650989886063\n",
      "Epoch --> 4 : 0.7641641930238975\n",
      "Validation Loss :  0.7601570966724152\n",
      "Epoch --> 5 : 0.7507465100230772\n",
      "Validation Loss :  0.7517233541087093\n",
      "Epoch --> 6 : 0.738889479853714\n",
      "Validation Loss :  0.7521697708365973\n",
      "Epoch --> 7 : 0.7291795091941066\n",
      "Validation Loss :  0.750002233688836\n",
      "Epoch --> 8 : 0.7198788620707427\n",
      "Validation Loss :  0.7469934388620313\n",
      "Epoch --> 9 : 0.712457402212828\n",
      "Validation Loss :  0.7444149389219238\n",
      "Epoch --> 10 : 0.7046318964033406\n",
      "Validation Loss :  0.7473530151669013\n",
      "Epoch --> 11 : 0.6976235789364459\n",
      "Validation Loss :  0.743473089588418\n",
      "Epoch --> 12 : 0.6908087807547666\n",
      "Validation Loss :  0.7503929966408759\n",
      "Epoch --> 13 : 0.6858488955026657\n",
      "Validation Loss :  0.7424962635354216\n",
      "Epoch --> 14 : 0.6799942090098104\n",
      "Validation Loss :  0.7461857929259617\n",
      "Epoch --> 15 : 0.674474870717268\n",
      "Validation Loss :  0.7477122474936962\n",
      "Epoch --> 16 : 0.6691091107938182\n",
      "Validation Loss :  0.7472434088547598\n",
      "Epoch --> 17 : 0.6656105481422668\n",
      "Validation Loss :  0.7675320944672761\n",
      "Epoch --> 18 : 0.6591718043011202\n",
      "Validation Loss :  0.7542550562940887\n",
      "Epoch --> 19 : 0.6552648637619571\n",
      "Validation Loss :  0.7519973795972958\n",
      "Epoch --> 20 : 0.6512970717445672\n",
      "Validation Loss :  0.7594286853461333\n",
      "Epoch --> 21 : 0.6463186627375702\n",
      "Validation Loss :  0.7668595398638669\n",
      "Epoch --> 22 : 0.6414436349998119\n",
      "Validation Loss :  0.765846122892622\n",
      "Epoch --> 23 : 0.6372566962294778\n",
      "Validation Loss :  0.7682680008135601\n",
      "Epoch --> 24 : 0.6337197695293239\n",
      "Validation Loss :  0.7582267376540113\n",
      "Epoch --> 25 : 0.62999451500557\n",
      "Validation Loss :  0.7806177387650532\n",
      "Epoch --> 26 : 0.6251856710017255\n",
      "Validation Loss :  0.7720253044535057\n",
      "Epoch --> 27 : 0.6223553858880423\n",
      "Validation Loss :  0.7923862466261198\n",
      "Epoch --> 28 : 0.6166545756496651\n",
      "Validation Loss :  0.795018259244515\n",
      "Epoch --> 29 : 0.6137450187165885\n",
      "Validation Loss :  0.7759987655244962\n",
      "Epoch --> 30 : 0.6110216700907048\n"
     ]
    }
   ],
   "source": [
    "# I have trained RNN model for 30 epochs. At each epoch I am testing my model on unseen validation data and keeping the model \n",
    "# with lowest loss on validation data.\n",
    "valid_loss_min_gru = np.Inf \n",
    "best_gru_model = None\n",
    "n_epochs = 30\n",
    "best_model_epoch_gru = None\n",
    "for epoch in range(n_epochs):\n",
    "    gru_model.train()\n",
    "    train_loss_gru = 0.0\n",
    "    \n",
    "    for i, (train_data, target) in enumerate(train_loader_seq):\n",
    "        train_data = train_data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer_gru.zero_grad()\n",
    "        output = gru_model(train_data.float())\n",
    "        loss = loss_fn_gru(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_gru.step()\n",
    "        train_loss_gru += loss.item()*train_data.size(0)\n",
    "\n",
    "    gru_model.eval()\n",
    "    val_loss, val_acc = evalAccuracySeq(gru_model,val_loader_seq)\n",
    "    print(\"Validation Loss : \", val_loss)\n",
    "    if valid_loss_min_gru>val_loss:\n",
    "        valid_loss_min_gru = val_loss\n",
    "        best_gru_model = copy.deepcopy(gru_model)\n",
    "        best_model_epoch_gru = epoch+1\n",
    "    train_loss_gru = train_loss_gru/len(train_loader_seq.dataset)\n",
    "    print(\"Epoch --> \"+str(epoch+1)+\" :\", train_loss_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b6496dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GRU model is :  66.48333333333333\n"
     ]
    }
   ],
   "source": [
    "# I am comparing the 2 models. One with lowest validation loss and one after training for given number of epochs\n",
    "gru_acc = max(evalAccuracy(gru_model,test_loader_seq),evalAccuracy(best_gru_model,test_loader_seq))\n",
    "print(\"Accuracy for GRU model is : \", gru_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46dbd2",
   "metadata": {},
   "source": [
    "## C) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b0c71079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference - 5\n",
    "# class LSTM -> inherits Module class and implements LSTM model \n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=20,num_layers = 1,num_classes=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out,_ = self.lstm(x,(h0,c0))\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a2cdace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_model = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cb9f8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used CrossEntropy as loss function and Adam optimizer with learning rate 0.001\n",
    "lstm_model = lstm_model.to(device)\n",
    "loss_fn_lstm = torch.nn.CrossEntropyLoss()\n",
    "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "08bbf066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss :  0.8598460981920362\n",
      "Epoch --> 1 : 0.9449802254802965\n",
      "Validation Loss :  0.8134103210962688\n",
      "Epoch --> 2 : 0.8296320334583842\n",
      "Validation Loss :  0.8099127588672563\n",
      "Epoch --> 3 : 0.7973930859119187\n",
      "Validation Loss :  0.7981440318631939\n",
      "Epoch --> 4 : 0.7756513978802201\n",
      "Validation Loss :  0.7752794862599112\n",
      "Epoch --> 5 : 0.7617303925371783\n",
      "Validation Loss :  0.7785191499522577\n",
      "Epoch --> 6 : 0.7474318179464029\n",
      "Validation Loss :  0.7660064006700802\n",
      "Epoch --> 7 : 0.7369827116642667\n",
      "Validation Loss :  0.760266359740713\n",
      "Epoch --> 8 : 0.7276806716121816\n",
      "Validation Loss :  0.7586528738953638\n",
      "Epoch --> 9 : 0.7188625254169891\n",
      "Validation Loss :  0.7564961961104224\n",
      "Epoch --> 10 : 0.7110380084430822\n",
      "Validation Loss :  0.7495433731268083\n",
      "Epoch --> 11 : 0.70138484895243\n",
      "Validation Loss :  0.7568781503534798\n",
      "Epoch --> 12 : 0.6943743809535778\n",
      "Validation Loss :  0.7612862069418188\n",
      "Epoch --> 13 : 0.6854527098762137\n",
      "Validation Loss :  0.7610923639490114\n",
      "Epoch --> 14 : 0.6802471411081897\n",
      "Validation Loss :  0.7505471139163322\n",
      "Epoch --> 15 : 0.6752571038433496\n",
      "Validation Loss :  0.7613220828777024\n",
      "Epoch --> 16 : 0.6684342876392423\n",
      "Validation Loss :  0.7713389622014754\n",
      "Epoch --> 17 : 0.6614185166474682\n",
      "Validation Loss :  0.764179432997092\n",
      "Epoch --> 18 : 0.6562074472979755\n",
      "Validation Loss :  0.7581971031600357\n",
      "Epoch --> 19 : 0.6501246542368718\n",
      "Validation Loss :  0.7818551755864755\n",
      "Epoch --> 20 : 0.6458683998543172\n",
      "Validation Loss :  0.755870524059787\n",
      "Epoch --> 21 : 0.64117485430343\n",
      "Validation Loss :  0.7673226903422522\n",
      "Epoch --> 22 : 0.6357150963749567\n",
      "Validation Loss :  0.7745751184346057\n",
      "Epoch --> 23 : 0.6298766217556304\n",
      "Validation Loss :  0.7670661081820241\n",
      "Epoch --> 24 : 0.6251389492007354\n",
      "Validation Loss :  0.7685798803286646\n",
      "Epoch --> 25 : 0.6196899007252324\n",
      "Validation Loss :  0.7817130763338452\n",
      "Epoch --> 26 : 0.6166199971858197\n",
      "Validation Loss :  0.7814831843579789\n",
      "Epoch --> 27 : 0.6103027417522219\n",
      "Validation Loss :  0.7901971648456917\n",
      "Epoch --> 28 : 0.6064158004644888\n",
      "Validation Loss :  0.7917905605277192\n",
      "Epoch --> 29 : 0.6022392654086642\n",
      "Validation Loss :  0.7848745081653712\n",
      "Epoch --> 30 : 0.6001430251662484\n",
      "Validation Loss :  0.8072425712012992\n",
      "Epoch --> 31 : 0.5918001058222158\n",
      "Validation Loss :  0.8029920337052512\n",
      "Epoch --> 32 : 0.5899289524449157\n",
      "Validation Loss :  0.7959735731140632\n",
      "Epoch --> 33 : 0.5848147302792096\n",
      "Validation Loss :  0.8303118767112634\n",
      "Epoch --> 34 : 0.5816757199567999\n",
      "Validation Loss :  0.8014717965544357\n",
      "Epoch --> 35 : 0.5767542025718037\n"
     ]
    }
   ],
   "source": [
    "# I have trained LSTM model for 35 epochs. At each epoch I am testing my model on unseen validation data and keeping the model \n",
    "# with lowest loss on validation data.\n",
    "valid_loss_min_lstm = np.Inf \n",
    "n_epochs = 35\n",
    "best_lstm_model = None\n",
    "best_model_epoch_lstm = None\n",
    "for epoch in range(n_epochs):\n",
    "    lstm_model.train()\n",
    "    train_loss_lstm = 0.0\n",
    "    \n",
    "    for i, (train_data, target) in enumerate(train_loader_seq):\n",
    "        train_data = train_data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer_lstm.zero_grad()\n",
    "        output = lstm_model(train_data.float())\n",
    "        loss = loss_fn_lstm(output, target)\n",
    "        loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "        train_loss_lstm += loss.item()*train_data.size(0)\n",
    "\n",
    "    lstm_model.eval()\n",
    "    val_loss, val_acc = evalAccuracySeq(lstm_model,val_loader_seq)\n",
    "    print(\"Validation Loss : \", val_loss)\n",
    "    if valid_loss_min_lstm>val_loss:\n",
    "        valid_loss_min_lstm = val_loss\n",
    "        best_lstm_model = copy.deepcopy(lstm_model)\n",
    "        best_model_epoch_lstm = epoch+1\n",
    "        \n",
    "    train_loss_lstm = train_loss_lstm/len(train_loader_seq.dataset)\n",
    "    print(\"Epoch --> \"+str(epoch+1)+\" :\", train_loss_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "914803ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for LSTM model is :  66.5\n"
     ]
    }
   ],
   "source": [
    "# I am comparing the 2 models. One with lowest validation loss and one after training for given number of epochs\n",
    "lstm_acc = max(evalAccuracy(lstm_model, test_loader_seq),evalAccuracy(best_lstm_model, test_loader_seq))\n",
    "print(\"Accuracy for LSTM model is : \",lstm_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9594e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab66e2df",
   "metadata": {},
   "source": [
    "<b><i>What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN?</i></b>\n",
    "\n",
    "<i>Ans. </i>The accuracy for simple RNN, GRU and LSTM are 64.075%, 66.483% and 66.5% respectively. On the basis of the accuracy, it can be said that LSTM is quite better than RNN and almost equivalent to GRU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73e79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367d6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56d9f8e5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html <br>\n",
    "[2] https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist <br>\n",
    "[3] https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html<br>\n",
    "[4] https://pytorch.org/docs/stable/generated/torch.nn.GRU.html <br>\n",
    "[5] https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bcb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
